# -*- coding: utf-8 -*-
"""news-articles_recommender-system

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1tlAl7sG6KCHXUGVt_md_c23aiPFn7qPY

# Data Understanding

Dataset yang digunakan dalam proyek ini adalah dataset "News Articles" yang diperoleh dari Kaggle. Anda dapat mengunduh dataset tersebut dari tautan berikut: [News Articles Dataset](https://www.kaggle.com/datasets/bavalpreet26/newsarticle/data).

## Variable in News Articles Dataset :

1. **Article_Id**: Pengenal unik untuk setiap artikel dalam dataset.
2. **Title**: Judul atau tajuk utama artikel berita, yang memberikan ringkasan atau deskripsi singkat tentang isinya.
3. **Author**: Nama penulis atau kontributor yang menulis artikel.
4. **Date**: Tanggal dan waktu ketika artikel diterbitkan atau terakhir diperbarui.
5. **Content**: Bagian utama artikel, yang berisi informasi, berita, atau analisis terperinci.
6. **URL**: URL web atau tautan untuk mengakses artikel lengkap secara online, biasanya dihosting di situs web publikasi.

# Prepare Dataset

## Download Dataset
"""

from google.colab import files
files.upload()

!mkdir -p ~/.kaggle
!cp kaggle.json ~/.kaggle/
!chmod 600 /root/.kaggle/kaggle.json

!kaggle datasets download -d bavalpreet26/newsarticle

!unzip '/content/newsarticle.zip'

"""## Import Library"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
# %matplotlib inline
import matplotlib.pyplot as plt
from wordcloud import WordCloud

import re
import nltk
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from nltk.stem import WordNetLemmatizer
nltk.download('punkt')
nltk.download('stopwords')
nltk.download('wordnet')

from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics.pairwise import cosine_similarity

"""## Load Dataset"""

articles = pd.read_csv('/content/news_articles.csv')
data = articles.copy()

data.head()

"""# Data Wrangling

## Data Assessing
"""

data.info()

"""## Data Cleaning"""

data.isna().sum()

data.drop(columns=['URL', 'Date'], axis=1, inplace=True)

data.dropna(inplace=True)

data.isna().sum()

data.duplicated().sum()

"""# Data Prepocessing"""

# Convert column names to lowercase for consistency and ease of access
data.columns = data.columns.str.lower()
articles.columns = articles.columns.str.lower()

# Define function to clean author column
def clean_author_names(authors):
    cleaned_authors = []
    for author in authors:
        # Remove additional information using regex pattern
        author = re.sub(r'\b(?:is a.*in female lead roles?.*|the movie has.*in female lead roles?.*|.*is a.*|.*in female lead roles?.*)\b', '', author)

        # Standardize format: Convert to lowercase and remove leading/trailing spaces
        author = author.lower().strip()

        # Handle variations and abbreviations
        if author == 'ians':
            author = 'indo-asian news service'

        cleaned_authors.append(author)

    return cleaned_authors

# Apply cleaning function to the 'author' column
data['author'] = clean_author_names(data['author'])

# Initialize WordNet Lemmatizer
lemmatizer = WordNetLemmatizer()

# Function to clean text
def clean_text(text):
    # Convert text to lowercase
    text = text.lower()
    # Remove punctuation and special characters
    text = re.sub(r'[^a-zA-Z\s]', '', text)
    # Tokenize text
    tokens = word_tokenize(text)
    # Remove stopwords
    stop_words = set(stopwords.words('english'))
    filtered_tokens = [word for word in tokens if word not in stop_words]
    # Lemmatize words
    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in filtered_tokens]
    # Join tokens back into a single string
    clean_text = ' '.join(lemmatized_tokens)
    return clean_text

# Clean the column
data['content'] = data['content'].apply(clean_text)
data['title'] = data['title'].apply(clean_text)

"""# EDA"""

pd.DataFrame(data['author'].value_counts())

pd.DataFrame(data['author'].value_counts().head(15))

data.shape

# Define a function to generate and display a word cloud from a given string of words
def wordCloud(words):

    # Generate the word cloud with specified parameters
    wordCloud = WordCloud(width=800, height=500, background_color='white', random_state=21, max_font_size=120).generate(words)

    plt.figure(figsize=(10, 7))
    plt.imshow(wordCloud, interpolation='bilinear')
    plt.axis('off')

# Concatenate all words from the 'content' column into a single string
all_words = ' '.join([text for text in data['content']])
wordCloud(all_words)

# Filter the DataFrame to select rows where the author is "Rohit KVN"
rohit_kvn_content = ' '.join(data[data['author'] == 'rohit kvn']['content'])

# Generate and display the word cloud for Rohit KVN's content
wordCloud(rohit_kvn_content)

# Filter the DataFrame to select rows where the author is "Rohit KVN"
rohit_kvn_content = ' '.join(data[data['author'] == 'besta shankar']['content'])

# Generate and display the word cloud for Rohit KVN's content
wordCloud(rohit_kvn_content)

data['title'].unique()

from collections import Counter

# Function to extract top 5 most common words from content
def top_words(content):
    words = word_tokenize(content.lower())
    word_count = Counter(words)
    top_5_words = [word for word, count in word_count.most_common(5)]
    return top_5_words

# Group data by author and apply the function to extract top words
top_words_data = data.groupby('author')['content'].apply(lambda x: top_words(' '.join(x))).reset_index()

# Rename the columns
top_words_data.columns = ['author', 'top_5_words']

top_words_data.sample(5)

"""# Content-based Filtering"""

# Concatenate the 'author', 'title', and 'content' columns to create a single text string representing each news article
news = data['author'] + ' ' + data['title'] + ' ' + data['content']

# Initialize TF-IDF vectorizer
tfidf_vectorizer = TfidfVectorizer(stop_words='english')

# Fit and transform the text data
tfidf_matrix = tfidf_vectorizer.fit_transform(news)

# Compute similarity scores
cosine_sim = cosine_similarity(tfidf_matrix, tfidf_matrix)

# Function to recommend similar articles
def content_based_recommendation(text, top_n=5):
    # Preprocess the query and convert it into a feature vector
    query_vector = tfidf_vectorizer.transform([text])

    # Calculate cosine similarity between the query vector and all articles
    sim_scores = cosine_similarity(query_vector, tfidf_matrix)

    # Rank articles based on similarity scores
    ranked_indices = sim_scores.argsort()[0][::-1]

    # Return top-N recommended articles and their similarity scores
    top_indices = ranked_indices[1:top_n+1]  # Exclude the query itself
    recommendations = data.iloc[top_indices]
    similarity_scores = sim_scores[0][top_indices]

    return recommendations, similarity_scores

"""# Getting Recommendation"""

# Input free text
free_text = 'horror movie recommendations'

# Generate content-based recommendations for the given free text query
recommendations, similarity_scores = content_based_recommendation(free_text)

recommendations_df = pd.DataFrame(recommendations)
recommendations_df

similarity_scores_df = pd.DataFrame({'article_id': recommendations_df['article_id'], 'similarity_score': similarity_scores})
similarity_scores_df

# Create DataFrame with article IDs and their similarity scores
recommendation_scores = pd.DataFrame({'article_id': recommendations['article_id'], 'similarity_score': similarity_scores})

# Merge recommendation scores with the articles DataFrame using the article_id column
articles_with_scores = pd.merge(articles, recommendation_scores, on='article_id', how='inner')

# Sort the DataFrame by similarity score in descending order
articles_with_scores.sort_values(by='similarity_score', ascending=False)

